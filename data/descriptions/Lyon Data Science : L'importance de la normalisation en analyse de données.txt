L’analyse de données est une tâche complexe, composée de multiples étapes successives : lecture des données, pré-traitements, extraction des caractéristiques, modélisation des caractéristiques et évaluation. La normalisation des données semble appartenir majoritairement à l’étape de pré-traitements, ce qui est faux, puisqu’on peut la retrouver aussi dans des tâches haut niveau comme la détection d’anomalies, l’apprentissage automatique, l’apprentissage profond, les tests statistiques, etc. De plus, l’étape de pré-traitements est souvent vue comme contraignante et peu valorisante. Il n’en est rien, car en réalité, c’est ici que se joue une grande part de l’efficacité de toute la chaine de traitements. La valeur ajoutée du data scientist est rarement dans l’étape maSuperMethode.apply(), mais plus souvent dans la manière de présenter les données à cette méthode.\nDans cet exposé, nous verrons dans un premier temps l’intérêt de la normalisation, puis les différentes manières de normaliser, et enfin les applications.\n\nQuentin Barthélemy\n\nSlide : https://www.slideshare.net/secret/qej6iuFlYu1gna